# Knowledges Flow trong RAGFlow

## ðŸ“Œ `knowledges` lÃ  gÃ¬?

`knowledges` lÃ  má»™t **list cÃ¡c Ä‘oáº¡n text (chunks)** Ä‘Æ°á»£c retrieve tá»« Knowledge Base (KB), dÃ¹ng Ä‘á»ƒ cung cáº¥p context cho LLM khi tráº£ lá»i cÃ¢u há»i.

## ðŸ”„ Flow hoÃ n chá»‰nh trong `chat()` function:

### 1ï¸âƒ£ **Khá»Ÿi táº¡o** (Line 525)
```python
knowledges = []
```
- Báº¯t Ä‘áº§u vá»›i list rá»—ng

### 2ï¸âƒ£ **Retrieval tá»« KB** (Lines 527-580)

**CÃ³ 2 cÃ¡ch láº¥y knowledges:**

#### **A. Deep Reasoning Mode** (Lines 530-552)
```python
if prompt_config.get("reasoning", False):
    reasoner = DeepResearcher(...)
    for think in reasoner.thinking(kbinfos, " ".join(questions)):
        if isinstance(think, str):
            thought = think
            knowledges = [t for t in think.split("\n") if t]
```
- DÃ¹ng AI Ä‘á»ƒ suy luáº­n sÃ¢u
- Táº¡o nhiá»u queries khÃ¡c nhau
- Tá»•ng há»£p káº¿t quáº£ thÃ nh knowledge chunks

#### **B. Standard Retrieval Mode** (Lines 554-580)
```python
else:
    if embd_mdl:
        kbinfos = retriever.retrieval(
            " ".join(questions),
            embd_mdl,
            tenant_ids,
            dialog.kb_ids,
            ...
        )
    knowledges = kb_prompt(kbinfos, max_tokens)
```
- **BÆ°á»›c 1:** Vector search trong Elasticsearch
  - Encode question thÃ nh vector
  - TÃ¬m chunks tÆ°Æ¡ng tá»±
  - Rerank náº¿u cÃ³ rerank model
  
- **BÆ°á»›c 2:** `kb_prompt()` format chunks
  - Láº¥y chunks tá»« `kbinfos["chunks"]`
  - Format thÃ nh text dá»… Ä‘á»c
  - Limit theo `max_tokens`

**Káº¿t quáº£:** `knowledges` lÃ  list string, má»—i string lÃ  1 chunk:
```python
knowledges = [
    "Chunk 1: Pháº­t giÃ¡o lÃ  gÃ¬...",
    "Chunk 2: BÃ¡t quan trai...",
    "Chunk 3: Tu táº­p..."
]
```

### 3ï¸âƒ£ **Check empty** (Lines 585-588)
```python
if not knowledges and prompt_config.get("empty_response"):
    empty_res = prompt_config["empty_response"]
    yield {"answer": empty_res, ...}
    return
```
- Náº¿u khÃ´ng tÃ¬m tháº¥y knowledge â†’ Tráº£ vá» empty response
- VD: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin..."

### 4ï¸âƒ£ **ThÃªm vÃ o System Prompt** (Line 590)
```python
kwargs["knowledge"] = "\n------\n" + "\n\n------\n\n".join(knowledges)
```

**Format:**
```
------
Chunk 1: Pháº­t giÃ¡o lÃ  gÃ¬...

------

Chunk 2: BÃ¡t quan trai...

------

Chunk 3: Tu táº­p...
```

Sau Ä‘Ã³ format vÃ o system prompt:
```python
system_content = prompt_config["system"].format(**kwargs)
```

**System prompt sáº½ cÃ³ dáº¡ng:**
```
Báº¡n lÃ  trá»£ lÃ½ AI...

## Knowledge Base:
------
Chunk 1: Pháº­t giÃ¡o lÃ  gÃ¬...

------

Chunk 2: BÃ¡t quan trai...
```

### 5ï¸âƒ£ **DÃ¹ng Ä‘á»ƒ generate citation** (Lines 614-615, 647-660)
```python
if knowledges and (prompt_config.get("quote", True) and kwargs.get("quote", True)):
    prompt4citation = citation_prompt()
```

**Citation prompt yÃªu cáº§u LLM:**
- TrÃ­ch dáº«n nguá»“n báº±ng `[ID:0]`, `[ID:1]`
- Map ID vá»›i chunk index

**Sau khi LLM tráº£ lá»i:**
```python
if knowledges and prompt_config.get("quote"):
    idx = set([])
    # Insert citations vÃ o answer
    answer, idx = retriever.insert_citations(
        answer,
        [ck["content_ltks"] for ck in kbinfos["chunks"]],
        [ck["vector"] for ck in kbinfos["chunks"]],
        embd_mdl,
        ...
    )
```

**Káº¿t quáº£:**
```
Answer text vá»›i citations [ID:0] vÃ  [ID:2]
```

## ðŸ“Š VÃ­ dá»¥ cá»¥ thá»ƒ:

### Input:
```
User: "BÃ¡t quan trai lÃ  gÃ¬?"
```

### Retrieval:
```python
kbinfos = {
    "chunks": [
        {
            "content_ltks": "bÃ¡t quan trai lÃ  giá»›i luáº­t...",
            "content_with_weight": "BÃ¡t quan trai lÃ  8 giá»›i luáº­t tu táº­p...",
            "similarity": 0.95,
            ...
        },
        {
            "content_ltks": "tu táº­p pháº­t phÃ¡p...",
            "content_with_weight": "Tu táº­p pháº­t phÃ¡p cáº§n cÃ³...",
            "similarity": 0.85,
            ...
        }
    ]
}

knowledges = kb_prompt(kbinfos, max_tokens=8192)
# Result:
knowledges = [
    "BÃ¡t quan trai lÃ  8 giá»›i luáº­t tu táº­p...",
    "Tu táº­p pháº­t phÃ¡p cáº§n cÃ³..."
]
```

### System Prompt:
```
HÃ´m nay lÃ  Thá»© TÆ°, ngÃ y 30, thÃ¡ng 10, nÄƒm 2025, lÃºc 14:30:00.

Báº¡n lÃ  trá»£ lÃ½ AI cá»§a Tháº§y ThÃ­ch Nháº¥t Háº¡nh...

## Knowledge Base:
------
BÃ¡t quan trai lÃ  8 giá»›i luáº­t tu táº­p...

------

Tu táº­p pháº­t phÃ¡p cáº§n cÃ³...

## Historical Memory:
User Ä‘Ã£ há»i vá» pháº­t phÃ¡p trÆ°á»›c Ä‘Ã³...
```

### LLM Response:
```
BÃ¡t quan trai lÃ  8 giá»›i luáº­t tu táº­p trong Pháº­t giÃ¡o [ID:0].
Khi tu táº­p, cáº§n cÃ³ lÃ²ng tin vÃ  tinh táº¥n [ID:1].
```

### Final Answer:
```
{
  "answer": "BÃ¡t quan trai lÃ  8 giá»›i luáº­t tu táº­p trong Pháº­t giÃ¡o [ID:0]...",
  "reference": {
    "chunks": [
      {"chunk_id": "...", "content": "BÃ¡t quan trai lÃ  8...", "doc_name": "Pháº­t PhÃ¡p.pdf"},
      {"chunk_id": "...", "content": "Tu táº­p pháº­t phÃ¡p...", "doc_name": "Tu Táº­p.pdf"}
    ],
    "doc_aggs": [
      {"doc_name": "Pháº­t PhÃ¡p.pdf", "count": 1},
      {"doc_name": "Tu Táº­p.pdf", "count": 1}
    ]
  }
}
```

## ðŸŽ¯ Táº¡i sao cáº§n `knowledges`?

### âœ… **1. Cung cáº¥p Context cho LLM:**
- LLM khÃ´ng biáº¿t ná»™i dung trong KB
- `knowledges` = "cheat sheet" cho LLM
- GiÃºp tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn dá»¯ liá»‡u tháº­t

### âœ… **2. Giáº£m Hallucination:**
- KhÃ´ng cÃ³ knowledge â†’ LLM tá»± bá»‹a
- CÃ³ knowledge â†’ LLM dá»±a trÃªn facts

### âœ… **3. Citation/TrÃ­ch dáº«n:**
- User biáº¿t info tá»« Ä‘Ã¢u
- CÃ³ thá»ƒ verify nguá»“n
- TÄƒng Ä‘á»™ tin cáº­y

### âœ… **4. Token Optimization:**
- Chá»‰ gá»­i relevant chunks
- KhÃ´ng gá»­i toÃ n bá»™ KB
- Fit trong context window

## ðŸ”§ Tuning Parameters:

### Sá»‘ lÆ°á»£ng chunks:
```python
dialog.top_n = 6  # Láº¥y top 6 chunks
dialog.top_k = 1024  # Search trong 1024 docs
```

### Similarity threshold:
```python
dialog.similarity_threshold = 0.2  # Min similarity
dialog.vector_similarity_weight = 0.3  # Vector vs Text weight
```

### Max tokens:
```python
knowledges = kb_prompt(kbinfos, max_tokens=8192)
```
- Auto truncate Ä‘á»ƒ fit context window

## ðŸ› Debug:

### Xem knowledges Ä‘Æ°á»£c retrieve:
```python
logging.debug("{}->{}".format(" ".join(questions), "\n->".join(knowledges)))
```

### Check empty knowledges:
```python
if not knowledges:
    print("No knowledge retrieved!")
```

### Check cache hit:
```python
if kbinfos.get("_cached"):
    print("Knowledge from cache!")
```

## ðŸ“ˆ Performance:

### Without Cache:
```
Retrieval: 1500-3000ms
â”œâ”€ Vector search: ~1000ms
â”œâ”€ Reranking: ~500ms
â””â”€ Format chunks: ~50ms
```

### With Cache:
```
Retrieval: 5-10ms (450x faster!)
â””â”€ Redis lookup: ~5ms
```

## ðŸŽ‰ Summary:

`knowledges` = **Context tá»« Knowledge Base** Ä‘Æ°á»£c:
1. âœ… Retrieve tá»« vector search
2. âœ… Rerank theo relevance
3. âœ… Format thÃ nh text chunks
4. âœ… ThÃªm vÃ o system prompt
5. âœ… LLM dÃ¹ng Ä‘á»ƒ generate answer
6. âœ… Insert citations vÃ o answer
7. âœ… Return vá»›i references

**Vai trÃ²:** Bridge giá»¯a Knowledge Base vÃ  LLM Ä‘á»ƒ táº¡o RAG (Retrieval-Augmented Generation)

---
**File:** `api/db/services/dialog_service.py`  
**Function:** `chat()`  
**Lines:** 525-660
